{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aaf879c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from colorama import Fore, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfa99a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60ae8a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "(50000, 2)\n",
      "After adding 100\n",
      "(100, 2)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "print(df.head())\n",
    "print(df.shape) \n",
    "print(\"After adding 100\")\n",
    "df = df.head(100)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee81113",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Lower Case</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10dc332f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtaking 3 reviews first\u001b[0m\n",
      "Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\n",
      "-------------------\n",
      "\u001b[34mAfter lower case\u001b[0m\n",
      "basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\n"
     ]
    }
   ],
   "source": [
    "print(Fore.RED+ \"taking 3 reviews first\"+ Style.RESET_ALL)\n",
    "print(df['review'][3])\n",
    "print(\"-------------------\")\n",
    "print(Fore.BLUE+\"After lower case\"+ Style.RESET_ALL)\n",
    "df['review'] = df['review'].str.lower()\n",
    "print(df['review'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c26a281",
   "metadata": {},
   "source": [
    "<span style=\"color:yellow\">Remove HTML tags</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad2386a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mAfter removing HTML tags\u001b[0m\n",
      "basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\n"
     ]
    }
   ],
   "source": [
    "print(Fore.RED + \"After removing HTML tags\"+ Style.RESET_ALL)\n",
    "\n",
    "import re # regular expression which is used for searching a pattern in a text\n",
    "\n",
    "def remove_html(text):\n",
    "    pattern = re.compile('<.*?>') # <.*?> is a pattern for html tags\n",
    "    return pattern.sub(r'', text) # r'' means replace with nothing\n",
    "\n",
    "df['review'] = df['review'].apply(remove_html)\n",
    "print(df['review'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62cdec",
   "metadata": {},
   "source": [
    "<span style = \"color:green\">Remove Urls</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a6e4362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAfter removing URL\u001b[0m\n",
      "basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\n"
     ]
    }
   ],
   "source": [
    "print(Fore.YELLOW + \"After removing URL\"+ Style.RESET_ALL)\n",
    "\n",
    "def remove_url(text):\n",
    "    pattern = re.compile(r'http\\S+|www\\S+')\n",
    "    return pattern.sub(r'', text)   \n",
    "\n",
    "df['review'] = df['review'].apply(remove_url)\n",
    "print(df['review'][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e092d0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check URL remove function\n",
      " \n"
     ]
    }
   ],
   "source": [
    "# lets check url remove\n",
    "print(\"Check URL remove function\")\n",
    "text =\" https://www.linkedin.com/home?original_referer=https%3A%2F%2Fwww%2Egoogle%2Ecom%2F&originalSubdomain=pk\"\n",
    "print(remove_url(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74daab8",
   "metadata": {},
   "source": [
    "<span style=\"color:cyan\">Remove Punctuation</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8a5c6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mAfter removing Punctuations\u001b[0m\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "Hello he said and went\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import string,time\n",
    "print(Fore.MAGENTA + \"After removing Punctuations\"+ Style.RESET_ALL)\n",
    "print(string.punctuation) # to see all punctuations that are available in string library\n",
    "\n",
    "exclude = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char, '')\n",
    "    return text\n",
    "\n",
    "text1 = \"Hello!!!, he said ---and went.\"\n",
    "start = time.time()\n",
    "print(remove_punctuation(text1))\n",
    "time1 = time.time() - start\n",
    "print(time1*50000) # to check how much time it will take to process the text1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "924a2b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using translate method\n",
      "Hello he said and went\n",
      "0.0\n",
      "\u001b[36mBefore removing punctuations\u001b[0m\n",
      "probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it's not preachy or boring. it just never gets old, despite my having seen it some 15 or more times in the last 25 years. paul lukas' performance brings tears to my eyes, and bette davis, in one of her very few truly sympathetic roles, is a delight. the kids are, as grandma says, more like \"dressed-up midgets\" than children, but that only makes them more fun to watch. and the mother's slow awakening to what's happening in the world and under her own roof is believable and startling. if i had a dozen thumbs, they'd all be \"up\" for this movie.\n",
      "\u001b[36mAfter removing punctuations\u001b[0m\n",
      "probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie\n"
     ]
    }
   ],
   "source": [
    "# another method to remove punctuations\n",
    "def remove_punctuation2(text):\n",
    "    return text.translate(str.maketrans('', '', exclude)) # translate method is used to replace a character with another character\n",
    "\n",
    "print(\"using translate method\")\n",
    "start1 = time.time()\n",
    "print(remove_punctuation2(text1))\n",
    "time2 = time.time() - start1\n",
    "print(time2*50000) # to check how much time it will take to process the text1\n",
    "\n",
    "# print(time1/time2) # time difference between two methods\n",
    "print(Fore.CYAN + \"Before removing punctuations\"+ Style.RESET_ALL)\n",
    "print(df['review'][5])\n",
    "df['review'] = df['review'].apply(remove_punctuation2)\n",
    "print(Fore.CYAN + \"After removing punctuations\"+ Style.RESET_ALL)\n",
    "print(df['review'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b05e4b",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\">Chat_conversation handle</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cfd887d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAfter removing chat conversation\u001b[0m\n",
      "For Your Information I go to university\n"
     ]
    }
   ],
   "source": [
    "print(Fore.GREEN + \"After removing chat conversation\"+ Style.RESET_ALL)\n",
    "\n",
    "chat_keywords = {\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"LOL\": \"Laugh Out Loud\",\n",
    "    \"OMG\": \"Oh My God\",\n",
    "    \"IDK\": \"I Donâ€™t Know\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IMHO\": \"In My Humble Opinion\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"TBH\": \"To Be Honest\",\n",
    "    \"SMH\": \"Shaking My Head\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"ROFL\": \"Rolling On the Floor Laughing\",\n",
    "    \"BFF\": \"Best Friends Forever\",\n",
    "    \"TMI\": \"Too Much Information\",\n",
    "    \"YOLO\": \"You Only Live Once\",\n",
    "    \"IDC\": \"I Donâ€™t Care\",\n",
    "    \"LMK\": \"Let Me Know\"\n",
    "}\n",
    "\n",
    "def chat_conversation(textChat):\n",
    "    new_text = []\n",
    "    for w in textChat.split():   # w mean word\n",
    "        if w.upper() in chat_keywords.keys():\n",
    "            new_text.append(chat_keywords[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return ' '.join(new_text)\n",
    "\n",
    "\n",
    "print(chat_conversation(\"FYI I go to university\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdcef16",
   "metadata": {},
   "source": [
    "<span style=\"color:yellow\">Incorrect text handling (Spelling correction)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf6acbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mAfter correcting the spelling mistakes\u001b[0m\n",
      "The move was good.I will definitely recommend it to my friends\n"
     ]
    }
   ],
   "source": [
    "print(Fore.LIGHTYELLOW_EX + \"After correcting the spelling mistakes\"+ Style.RESET_ALL)\n",
    "\n",
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "text = \"The movi was good.I will defnitely recomend it to my freinds\"\n",
    "corrected = \" \".join([spell(word) for word in text.split()])\n",
    "print(corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010458a8",
   "metadata": {},
   "source": [
    "<span style = \"color:pink\">Stop words</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2a1531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95mAfter removing stop words\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\dell\n",
      "[nltk_data]     R\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n",
      "Length of stop words list 198\n",
      "This is a good movie. I will recommend it to my friends\n",
      "For actual data\n",
      "probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie\n"
     ]
    }
   ],
   "source": [
    "print(Fore.LIGHTMAGENTA_EX + \"After removing stop words\"+ Style.RESET_ALL)\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)\n",
    "print(\"Length of stop words list\", len(stop_words))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w not in stop_words:\n",
    "            new_text.append(w)\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    x = new_text[:]  # make a copy of new_text because we cannot change the list while iterating over it\n",
    "     # so we make a copy of it and iterate over the copy\n",
    "    new_text.clear() # clear the new_text list\n",
    "     # now we will add the words which are not in stop_words to new_text list\n",
    "    return ' '.join(x) # join the list to make a string\n",
    "print(remove_stopwords(\"This is a good movie. I will recommend it to my friends\"))\n",
    "\n",
    "print(\"For actual data\")\n",
    "df['review'] = df['review'].apply(remove_stopwords)\n",
    "print(df['review'][5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17faad5",
   "metadata": {},
   "source": [
    "<span style=\"color:yellow\">Remove Emojis</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eac52216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mAfter removing Emojis\u001b[0m\n",
      "This is a good movie. I will recommend it to my friends \n"
     ]
    }
   ],
   "source": [
    "print(Fore.LIGHTGREEN_EX + \"After removing Emojis\"+ Style.RESET_ALL) \n",
    "\n",
    "import re\n",
    "\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text) # no emoji will be left\n",
    "\n",
    "text = \"This is a good movie. I will recommend it to my friends ðŸ˜ŠðŸ˜‚\"\n",
    "print(remove_emojis(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40089fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a good movie. I will recommend it to my friends :smiling_face_with_smiling_eyes::face_with_tears_of_joy:\n"
     ]
    }
   ],
   "source": [
    "# we know that chatGpt understand emojis so we in that case we will extract the meaning of emojis\n",
    "import emoji\n",
    "print(emoji.demojize(\"This is a good movie. I will recommend it to my friends ðŸ˜ŠðŸ˜‚\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d0b4b1",
   "metadata": {},
   "source": [
    "<span style=\"color:blue\"> Tokenization</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0958018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'going', 'to', 'UET', 'Peshawar']\n",
      "['i am going to UET, Peshawar', ' I will meet my friends there', 'And we will do Project togather', '']\n"
     ]
    }
   ],
   "source": [
    "#  word level tokenization\n",
    "\n",
    "sent1 = 'i am going to UET Peshawar'\n",
    "print(sent1.split()) # split method will split the sentence into words based on space \n",
    "\n",
    "# sentence level tokenization \n",
    "sent2 = \"i am going to UET, Peshawar. I will meet my friends there.And we will do Project togather.\"\n",
    "print(sent2.split('.')) # split method will split the sentence into sentences based on full stop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83274d09",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">Regular expression based tokenization</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe63d8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'going', 'to', 'UET', 'Peshawar']\n",
      "['My name is Syed Talha Ali Shah', 'I am a student of Computer Engnieering', '\\nI am doing Genrative Ai', 'Ai is all about mathematics and programming', '\\nMostly we use mathematics in Ai', '']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\dell  R\\AppData\\Local\\Temp\\ipykernel_5416\\3769057191.py:3: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  tokens = re.findall(\"[\\w']+\", sent3) # \\w means word character (a-z, A-Z, 0-9, _), + means one or more, ' is for contractions like don't\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sent3 = 'I am going to UET Peshawar.'\n",
    "tokens = re.findall(\"[\\w']+\", sent3) # \\w means word character (a-z, A-Z, 0-9, _), + means one or more, ' is for contractions like don't\n",
    "print(tokens)\n",
    "\n",
    "text5 = \"\"\"My name is Syed Talha Ali Shah.I am a student of Computer Engnieering.\n",
    "I am doing Genrative Ai.Ai is all about mathematics and programming.\n",
    "Mostly we use mathematics in Ai.\"\"\"\n",
    "\n",
    "sentence = re.compile(\"[.!?]\").split(text5)\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298978bf",
   "metadata": {},
   "source": [
    "<span style=\"color:yellow\">NLTK based tokenization</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84264860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\dell\n",
      "[nltk_data]     R\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\dell\n",
      "[nltk_data]     R\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Don't\", 'split', 'contractions', 'like', \"can't\", 'or', \"won't\"]\n",
      "['I', 'am', 'Computer', 'Engineer', '.']\n",
      "['My name is Syed Talha Ali Shah.', 'I am a student of Computer Engineering.', 'I am doing Generative AI.', 'AI is all about mathematics and programming.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download required data\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "sent3 = \"Don't split contractions like can't or won't\"\n",
    "tokens = re.findall(r\"[\\w']+\", sent3)  # raw string fixes warning\n",
    "print(tokens)\n",
    "\n",
    "sent4 = \"I am Computer Engineer.\"\n",
    "print(word_tokenize(sent4))  # word level tokenization\n",
    "\n",
    "sent5 = \"\"\"My name is Syed Talha Ali Shah. I am a student of Computer Engineering.\n",
    "I am doing Generative AI. AI is all about mathematics and programming.\"\"\"\n",
    "\n",
    "print(sent_tokenize(sent5))  # sentence level tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdfc851",
   "metadata": {},
   "source": [
    "<span style = \"color:blue\">Spacy based tokenization</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03231944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (0.17.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.3.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dell  r\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\dell  R\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\dell  R\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\dell  R\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\dell  R\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\dell  R\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\dell  R\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa089e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy<2\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.3\n",
      "    Uninstalling numpy-2.3.3:\n",
      "      Successfully uninstalled numpy-2.3.3\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\dell  R\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\dell  R\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\dell  R\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (C:\\Users\\dell  R\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install \"numpy<2\" --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52db3dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in doc1:\n",
      "I\n",
      "am\n",
      "Computer\n",
      "Engineer\n",
      ".\n",
      "\n",
      "Tokens in doc2:\n",
      "My\n",
      "name\n",
      "is\n",
      "Syed\n",
      "Talha\n",
      "Ali\n",
      "Shah\n",
      ".\n",
      "\n",
      "Tokens in doc3:\n",
      "GenAi\n",
      "is\n",
      "best\n",
      "Field\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English model (make sure it's installed)\n",
    "# Run this once in your terminal if not installed:\n",
    "# python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Example texts\n",
    "doc1 = nlp(\"I am Computer Engineer.\")\n",
    "doc2 = nlp(\"My name is Syed Talha Ali Shah.\")\n",
    "doc3 = nlp(\"GenAi is best Field\")\n",
    "\n",
    "# Print tokens from doc1\n",
    "print(\"Tokens in doc1:\")\n",
    "for token in doc1:\n",
    "    print(token.text)\n",
    "\n",
    "# Print tokens from doc2\n",
    "print(\"\\nTokens in doc2:\")\n",
    "for token in doc2:\n",
    "    print(token.text)\n",
    "\n",
    "# Print tokens from doc3\n",
    "print(\"\\nTokens in doc3:\")\n",
    "for token in doc3:\n",
    "    print(token.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d669adee",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Stemmer</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14318a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b57c64f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return ' '.join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e04ceaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = \"Walk walks walking walked\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bdd4c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is sy talha ali shah. i am a student of comput engineering.i want to becom an ai engnineer.now my engneer is in progress.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"My name is Syed Talha Ali Shah. I am a student of Computer Engineering.I want to become an Ai Engnineer.Now my engneering is in Progress.\"\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb5308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
